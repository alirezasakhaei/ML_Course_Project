{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Project_98101714.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j-HOlrSag-xi",
        "R_ys5sPwoOeH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-HOlrSag-xi"
      },
      "source": [
        "# IMPORTS AND LOADS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mh1Q-JOvgz5M"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import numpy\n",
        "from scipy.signal import periodogram\n",
        "import scipy.io as io\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier as  RFC\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "import sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.base import clone\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krtD3oN_g7kT",
        "outputId": "d6104855-fd29-405f-8537-fc3690e575af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following arrays are used to determine the letter after the prediction of the model:"
      ],
      "metadata": {
        "id": "CI7Sr47sRWvE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEuM4DBZg9Oi"
      },
      "outputs": [],
      "source": [
        "DICT_1D = []\n",
        "for i in range(26):\n",
        "    DICT_1D.append(chr(i + 65))\n",
        "for i in range(10):\n",
        "    DICT_1D.append(i)\n",
        "DICT_1D = np.array(DICT_1D)\n",
        "DICT_2D = DICT_1D.reshape(6, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "enRfxRAlg-S7"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/Data/Train/TrainData'\n",
        "test_path = '/content/drive/MyDrive/Data/Test/TestData'\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for i in range(9):\n",
        "    temp = io.loadmat(train_path + str(i+1) + '.mat')\n",
        "    train_data.append(temp['TrainData'+ str(i+1)])\n",
        "\n",
        "    temp = io.loadmat(test_path + str(i+1) + '.mat')\n",
        "    test_data.append(temp['TestData' + str(i+1)])\n",
        "\n",
        "    del temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7S5k589eecvU"
      },
      "outputs": [],
      "source": [
        "Ts = train_data[0][0][1] - train_data[0][0][0]\n",
        "Fs = 1 / Ts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Primary Classes"
      ],
      "metadata": {
        "id": "H8pi0PjtIoGG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_ys5sPwoOeH"
      },
      "source": [
        "## 1.1 Feature Operations Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "0P8SvZuLoThV"
      },
      "outputs": [],
      "source": [
        "class Feature():\n",
        "    \n",
        "    def __init__(self, person):\n",
        "        self.person = person\n",
        "\n",
        "        # Features related to only one channel\n",
        "\n",
        "        self.lens_ch = np.array([1, 1, 1, person.window, 1, 1, 1, person.window // 2, 1, 1, 7, 1])\n",
        "        self.funcs_ch = [Feature.time_mean,\n",
        "                         Feature.time_var,\n",
        "                         Feature.time_skew,\n",
        "                         Feature.stack_time,\n",
        "                         Feature.fft_mean,\n",
        "                         Feature.fft_var,\n",
        "                         Feature.fft_skew,\n",
        "                         Feature.stack_fft,\n",
        "                         Feature.ft_median,                      \n",
        "                         Feature.ft_mean_freq,\n",
        "                         Feature.bandpower,\n",
        "                         Feature.f_max]\n",
        "\n",
        "        # Features that are related to the relation of more than one channel\n",
        "\n",
        "        self.lens_cs = np.array([1, 1])\n",
        "        self.funcs_cs = [Feature.time_cor,\n",
        "                         Feature.fft_cor]\n",
        "\n",
        "        self.funcs = self.funcs_ch + self.funcs_cs\n",
        "\n",
        "\n",
        "    ####################################\n",
        "    # These are different models with different hyperparameters\n",
        "    # Last function calls each one and gets the best accuracy\n",
        "    # Using accuracy as the metric, best classifier is chosen\n",
        "\n",
        "    def logistic(self):\n",
        "        \n",
        "        clf = LR(class_weight= {0: 1, 1:person.weight}, max_iter= 5000)\n",
        "        top_features = np.zeros(3) - 1\n",
        "        for k in range(3):\n",
        "            max_acc = 0\n",
        "            for i in range(14 - k):\n",
        "\n",
        "                features = np.zeros(14).astype(np.int64)\n",
        "                if np.where(top_features > -1)[0].shape[0] > 0:\n",
        "                    features[top_features[np.where(top_features > -1)[0]].astype(np.int64)] = 1\n",
        "                zeros_indices = np.where(features == 0)[0]\n",
        "                features[zeros_indices[i]] = 1\n",
        "                data = self.feature_maker(person.X_train, features)\n",
        "                acc = person.cross_validation(data, clf)\n",
        "\n",
        "                if acc > max_acc:\n",
        "                    max_acc = acc\n",
        "                    temp = int(zeros_indices[i])\n",
        "\n",
        "            top_features[k] = temp\n",
        "\n",
        "        return (max_acc, top_features, clf)\n",
        "\n",
        "\n",
        "    def svm(self):\n",
        "\n",
        "        max_acc_hyperparams = 0\n",
        "        top_clf = None\n",
        "        top_top_features = None\n",
        "\n",
        "        Cs = [0.3]\n",
        "        for C in Cs:\n",
        "            clf = SVC(class_weight= {0: 1, 1:person.weight}, max_iter= 5000, C= C)\n",
        "            top_features = np.zeros(3) - 1\n",
        "            for k in range(3):\n",
        "                max_acc = 0\n",
        "                for i in range(14 - k):\n",
        "\n",
        "                    features = np.zeros(14).astype(np.int64)\n",
        "                    if np.where(top_features > -1)[0].shape[0] > 0:\n",
        "                        features[top_features[np.where(top_features > -1)[0]].astype(np.int64)] = 1\n",
        "                    zeros_indices = np.where(features == 0)[0]\n",
        "                    features[zeros_indices[i]] = 1\n",
        "                    data = self.feature_maker(person.X_train, features)\n",
        "                    acc = person.cross_validation(data, clf)\n",
        "\n",
        "                    if acc > max_acc:\n",
        "                        max_acc = acc\n",
        "                        temp = int(zeros_indices[i])\n",
        "\n",
        "                top_features[k] = temp\n",
        "            \n",
        "            if max_acc > max_acc_hyperparams:\n",
        "\n",
        "                max_acc_hyperparams = max_acc\n",
        "                top_clf = clf\n",
        "                top_top_features = top_features\n",
        "\n",
        "        return (max_acc_hyperparams, top_top_features, clf)\n",
        "\n",
        "    def Random_Forest(self):\n",
        "\n",
        "        max_acc_hyperparams = 0\n",
        "        top_clf = None\n",
        "        top_top_features = None\n",
        "\n",
        "        n_estimators = [2, 5, 8]\n",
        "        max_depths = [2, 5, 8]\n",
        "\n",
        "        for n_estimator in n_estimators:\n",
        "            for max_depth in max_depths:\n",
        "                clf = RFC(class_weight= {0: 1, 1:person.weight}, n_estimators=n_estimator, max_depth=max_depth)\n",
        "                top_features = np.zeros(3) - 1\n",
        "                for k in range(3):\n",
        "                    max_acc = 0\n",
        "                    for i in range(14 - k):\n",
        "\n",
        "                        features = np.zeros(14).astype(np.int64)\n",
        "                        if np.where(top_features > -1)[0].shape[0] > 0:\n",
        "                            features[top_features[np.where(top_features > -1)[0]].astype(np.int64)] = 1\n",
        "                        zeros_indices = np.where(features == 0)[0]\n",
        "                        features[zeros_indices[i]] = 1\n",
        "                        data = self.feature_maker(person.X_train, features)\n",
        "                        acc = person.cross_validation(data, clf)\n",
        "\n",
        "                        if acc > max_acc:\n",
        "                            max_acc = acc\n",
        "                            temp = int(zeros_indices[i])\n",
        "\n",
        "                    top_features[k] = temp\n",
        "                \n",
        "                if max_acc > max_acc_hyperparams:\n",
        "\n",
        "                    max_acc_hyperparams = max_acc\n",
        "                    top_clf = clf\n",
        "                    top_top_features = top_features\n",
        "\n",
        "        return (max_acc_hyperparams, top_top_features, clf)\n",
        "\n",
        "    def tune(self):\n",
        "\n",
        "        person = self.person\n",
        "        MAX_ACC = 0\n",
        "        CLF = None\n",
        "        TOP_FEATURES = np.zeros(3)\n",
        "\n",
        "\n",
        "        acc_lr, top_features_lr, clf_lr = self.logistic()\n",
        "        acc_svm, top_features_svm, clf_svm = self.svm()\n",
        "        acc_rf, top_features_rf, clf_rf = self.Random_Forest()\n",
        "\n",
        "        # Comparison between classifier\n",
        "        if acc_lr > MAX_ACC:\n",
        "            MAX_ACC = acc_lr\n",
        "            CLF = clf_lr\n",
        "            TOP_FEATURES = top_features_lr\n",
        "\n",
        "        if acc_svm > MAX_ACC:\n",
        "            MAX_ACC = acc_svm\n",
        "            CLF = clf_svm\n",
        "            TOP_FEATURES = top_features_svm\n",
        "\n",
        "        if acc_rf > MAX_ACC:\n",
        "            MAX_ACC = acc_rf\n",
        "            CLF = clf_rf\n",
        "            TOP_FEATURES = top_features_rf\n",
        "        \n",
        "\n",
        "        person.TOP_FEATURES = TOP_FEATURES\n",
        "        person.CLF = CLF\n",
        "        names = [self.funcs[int(TOP_FEATURES[0])].__name__ , self.funcs[int(TOP_FEATURES[1])].__name__, self.funcs[int(TOP_FEATURES[2])].__name__]\n",
        "        person.TOP_FEATURES_NAMES = ' '.join(names)\n",
        "        person.MAX_ACC = MAX_ACC\n",
        "\n",
        "    #########################################################\n",
        "\n",
        "    #########################################################\n",
        "    # Theses functions are used to create features from the data\n",
        "    # Some are only related to one channel\n",
        "    # and some are between channels\n",
        "\n",
        "    def feature_maker(self, X, selection):\n",
        "\n",
        "        selection = np.array(selection)\n",
        "        select_channels_wise = selection[:len(self.funcs_ch)]\n",
        "        # print(len(self.funcs_ch))\n",
        "        select_cross_channel = selection[len(self.funcs_ch):]\n",
        "\n",
        "        # print(select_channels_wise)\n",
        "        # print(select_cross_channel)\n",
        "\n",
        "        data1 = self.channel_wise(X, select_channels_wise)\n",
        "        data2 = self.cross_channel(X, select_cross_channel)\n",
        "\n",
        "        data = np.hstack((data1, data2))\n",
        "        return data\n",
        "\n",
        "    def cross_channel(self, X, select):\n",
        "\n",
        "        data = np.zeros((int(self.person.trials), int(28 * np.sum(np.multiply(select, self.lens_cs)))))\n",
        "        for i in range(self.person.trials):\n",
        "            index = 0\n",
        "            for j in range(len(self.funcs_cs)):\n",
        "                if select[j] == 1:\n",
        "                    for k in range(8):\n",
        "                        for s in range(k+1, 8):                           \n",
        "                            data[i][index + self.lens_cs[j] * k : index + self.lens_cs[j] * (k+1)] = self.funcs_cs[j](self.person, X[i][k], X[i][s])\n",
        "                    index += 28 * self.lens_cs[j]\n",
        "\n",
        "        return data\n",
        "\n",
        "    def channel_wise(self, X, select):\n",
        "\n",
        "        # print(select)\n",
        "        data = np.zeros((int(self.person.trials), int(8 * np.sum(np.multiply(select, self.lens_ch)))))\n",
        "        for i in range(self.person.trials):\n",
        "            index = 0\n",
        "            for j in range(len(self.funcs_ch)):\n",
        "                if select[j] == 1:\n",
        "                    \n",
        "                    for k in range(8):\n",
        "                        x = X[i][k]\n",
        "                        if not self.funcs_ch[j].__name__ == 'time_mean':\n",
        "                            x = x - np.mean(x)\n",
        "                        data[i][index + self.lens_ch[j] * k : index + self.lens_ch[j] * (k+1)] = self.funcs_ch[j](self.person, x)\n",
        "                    index += 8 * self.lens_ch[j]\n",
        "        return data\n",
        "\n",
        "    #############################################################\n",
        "    #############################################################\n",
        "    # Features: \n",
        "\n",
        "    # Channel Wise:\n",
        "    def time_mean(person, x):\n",
        "        return np.mean(x)\n",
        "\n",
        "    def time_var(person, x):\n",
        "        return np.var(x)\n",
        "\n",
        "    def time_skew(person, x):\n",
        "        return scipy.stats.skew(x)\n",
        "\n",
        "    def fft_mean(person, x):\n",
        "        fft = np.real(scipy.fft.fft(x) / person.fs)\n",
        "        return np.mean(fft)\n",
        "\n",
        "    def fft_var(person, x):\n",
        "        fft = np.real(scipy.fft.fft(x) / person.fs)\n",
        "        return np.var(fft)\n",
        "\n",
        "    def fft_skew(person, x):\n",
        "        fft = np.real(scipy.fft.fft(x) / person.fs)\n",
        "        return np.var(fft)\n",
        "\n",
        "    def ft_median(person, x):\n",
        "        f, Pxx = periodogram(x, fs= person.fs)\n",
        "        sum = np.trapz(Pxx, f)\n",
        "        for i in range(f.shape[0]):\n",
        "            temp = np.trapz(Pxx[:i], f[:i])\n",
        "            if temp > sum / 2:\n",
        "                break\n",
        "        return f[i]\n",
        "    def ft_mean_freq(person, x):\n",
        "        f, Pxx = periodogram(x, fs= person.fs)\n",
        "        a = np.trapz(np.multiply(f, Pxx), f)\n",
        "        b = np.trapz(Pxx, f)\n",
        "        return a/b\n",
        "\n",
        "    def bandpower(person, x):\n",
        "        f, Pxx = periodogram(x, fs= person.fs)\n",
        "        bands = np.zeros(7)\n",
        "        for i in range(7):\n",
        "            fmin = 2 + 7*i\n",
        "            fmax = 2 + 7*(i+1)\n",
        "            ind_min = np.argmax(f > fmin) - 1\n",
        "            ind_max = np.argmax(f > fmax) - 1\n",
        "            bands[i] = np.trapz(Pxx[ind_min: ind_max], f[ind_min: ind_max])\n",
        "\n",
        "        return bands\n",
        "\n",
        "    def f_max(person, x):\n",
        "        f, Pxx = periodogram(x, fs= person.fs)\n",
        "        index = np.argmax(Pxx)\n",
        "        return(f[index])\n",
        "\n",
        "\n",
        "    def stack_fft(person, x):\n",
        "        return np.real(scipy.fft.fft(x) / person.fs)[person.window // 2:]\n",
        "\n",
        "    def stack_time(person, x):\n",
        "        return x \n",
        "\n",
        "\n",
        "    # Cross Channel featurs\n",
        "\n",
        "    def time_cor(person, x, y):\n",
        "        return np.correlate(x,y) / np.sqrt(np.correlate(x,x) * np.correlate(y,y))\n",
        "        return np.correlate(x, y)\n",
        "\n",
        "    def fft_cor(person, x, y):\n",
        "      \n",
        "        return np.correlate(np.real(scipy.fft.fft(x) / person.fs), np.real(scipy.fft.fft(y) / person.fs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reAMkh-VWMCu"
      },
      "source": [
        "## 1.2 Person Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "hfulCYxDWNq_"
      },
      "outputs": [],
      "source": [
        "class Person:\n",
        "\n",
        "    def __init__(self, train_data, test_data, window= 128, fs= 256):\n",
        "\n",
        "        self.fs = fs \n",
        "        self.train_data = train_data\n",
        "        self.test_data = test_data\n",
        "        self.EEG_train = train_data[1:9]\n",
        "        self.EEG_test = test_data[1:9]\n",
        "        self.light_train = train_data[9]\n",
        "        self.light_test = test_data[9]\n",
        "        self.label_train = train_data[10]\n",
        "        if np.sum(train_data[10]) == 300: # Single Char\n",
        "            self.trials = 2700\n",
        "            self.single_char = True\n",
        "            self.weight = 35\n",
        "        else:\n",
        "            self.trials = 900 # Row-Column\n",
        "            self.single_char = False\n",
        "            self.weight = 5\n",
        "\n",
        "        self.channels = 8\n",
        "        self.window = window \n",
        "        \n",
        "        self.extract_data()\n",
        "\n",
        "    def extract_data(self):\n",
        "        \n",
        "        self.X_train = np.zeros((self.trials, self.channels, self.window))\n",
        "        self.X_test = np.zeros((self.trials, self.channels, self.window))\n",
        "        self.y_train = np.zeros(self.trials)\n",
        "        self.letters_train = np.zeros(self.trials)\n",
        "        self.letters_test = np.zeros(self.trials)\n",
        "\n",
        "        i = 0\n",
        "        for j in range(1, self.light_train.shape[0]):\n",
        "            if self.light_train[j-1] == 0 and not self.light_train[j] == 0:\n",
        "                index0 = (j-(self.window//8))\n",
        "                index1 = (j+(7*self.window//8))\n",
        "                for k in range(self.channels):\n",
        "                    self.X_train[i][k] =  self.EEG_train[k][index0:index1]\n",
        "                self.y_train[i] = self.label_train[j] \n",
        "                self.letters_train[i] = self.light_train[j]\n",
        "                i += 1\n",
        "\n",
        "        i = 0\n",
        "        for j in range(1, self.light_test.shape[0]):\n",
        "            if self.light_test[j-1] == 0 and not self.light_test[j] == 0:\n",
        "                index0 = j-(self.window//8)\n",
        "                index1 = j+(7*self.window//8)\n",
        "                for k in range(self.channels):\n",
        "                    self.X_test[i][k] =  self.EEG_test[k][index0:index1]         \n",
        "                self.letters_test[i] = self.light_test[j]\n",
        "                i += 1\n",
        "\n",
        "    def plot_sample(self):\n",
        "\n",
        "        plt.figure(figsize= (20, 15))\n",
        "        for k in range(1, 9):\n",
        "            plt.subplot(8, 1, k)\n",
        "            plt.xlabel('t')\n",
        "            plt.ylabel('Channel ' + str(k))\n",
        "            plt.grid(True)\n",
        "            plt.plot(self.X_train[k][0])\n",
        "\n",
        "    def unison_shuffled_copies(a, b):\n",
        "        assert len(a) == len(b)\n",
        "        p = np.random.permutation(len(a))\n",
        "        return a[p], b[p]\n",
        "\n",
        "    # This method is used in training session: \n",
        "\n",
        "    def data_generator(self, Train= True):\n",
        "\n",
        "        if Train:\n",
        "            X = self.X_train\n",
        "\n",
        "        else:\n",
        "            X = self.X_test\n",
        "\n",
        "        feature = Feature(self) # Feature Object\n",
        "        features = np.zeros(14) # 14 Features are avaiable\n",
        "        features[self.TOP_FEATURES.astype(np.int64)] = 1\n",
        "        data = feature.feature_maker(X, features)\n",
        "\n",
        "        return data\n",
        "\n",
        "    # This method is used in Hyperparameter tuning session: \n",
        "    def tune(self):\n",
        "\n",
        "        feature = Feature(self)\n",
        "        feature.tune()\n",
        "\n",
        "    def cross_validation(self, data, clf):\n",
        "\n",
        "        data = StandardScaler().fit(data).transform(data)\n",
        "        accuracies = []\n",
        "\n",
        "        # CROSS VALIDATION\n",
        "        for i in range(5): \n",
        "\n",
        "            indexes = np.arange(self.trials)\n",
        "            test_index = np.arange(self.trials//5 * i, self.trials//5 * (i+1))\n",
        "            train_index = np.setdiff1d(indexes, test_index)\n",
        "\n",
        "            X_train = data[train_index]\n",
        "            y_train = self.y_train[train_index]\n",
        "            letters_train = self.letters_train[train_index]\n",
        "\n",
        "            X_test = data[test_index]\n",
        "            y_test = self.y_train[test_index]\n",
        "            letters_test = self.letters_train[test_index]\n",
        "\n",
        "            d, y = Person.unison_shuffled_copies(X_train, y_train)        \n",
        "            clf.fit(d, y)\n",
        "            pred = clf.predict(X_test)\n",
        "\n",
        "            # Weighted Accuracy\n",
        "            sw = np.multiply(y_test, (self.weight - 1)*np.ones(y_test.shape[0])) + np.ones(y_test.shape[0])\n",
        "            accuracies.append(np.round(acc(y_test, pred, sample_weight= sw), 2))\n",
        "\n",
        "        return np.mean(np.array(accuracies))\n",
        "\n",
        "    def predict_on_train(self):\n",
        "\n",
        "        data = self.data_generator()\n",
        "        data = StandardScaler().fit(data).transform(data)\n",
        "        word = []\n",
        "        accuracies = []\n",
        "\n",
        "        # CROSS VALIDATION\n",
        "        for i in range(5): \n",
        "\n",
        "            indexes = np.arange(self.trials)\n",
        "            test_index = np.arange(self.trials//5 * i, self.trials//5 * (i+1))\n",
        "            train_index = np.setdiff1d(indexes, test_index)\n",
        "\n",
        "            X_train = data[train_index]\n",
        "            y_train = self.y_train[train_index]\n",
        "            letters_train = self.letters_train[train_index]\n",
        "\n",
        "            X_test = data[test_index]\n",
        "            y_test = self.y_train[test_index]\n",
        "            letters_test = self.letters_train[test_index]\n",
        "            clf = clone(self.CLF)\n",
        "            d, y = Person.unison_shuffled_copies(X_train, y_train)        \n",
        "            clf.fit(d, y)\n",
        "            pred = clf.predict(X_test)\n",
        "\n",
        "            # Weighted Accuracy\n",
        "            sw = np.multiply(y_test, (self.weight - 1)*np.ones(y_test.shape[0])) + np.ones(y_test.shape[0])\n",
        "            accuracies.append(np.round(acc(y_test, pred, sample_weight= sw), 2))\n",
        "\n",
        "            # Finding the letter:\n",
        "            simu = np.multiply(pred, letters_test).astype(np.int64)\n",
        "            simu[np.where(simu == 0)[0][0]] = 36\n",
        "            count = np.bincount(simu)\n",
        "            count[0] = 0\n",
        "            count[-1] = count[-1] - 1\n",
        "\n",
        "            if self.single_char:\n",
        "                confidence = count[np.argmax(count)] / np.sum(count)\n",
        "                letter = np.argmax(count) - 1\n",
        "                word.append(DICT_1D[letter])\n",
        "\n",
        "            else:\n",
        "                if not np.sum(count[13:]) == 0:\n",
        "                    raise ValueError\n",
        "\n",
        "                count_rows = count[7:12]\n",
        "                count_columns = count[1:6]\n",
        "\n",
        "                row = np.argmax(count_rows)\n",
        "                col = np.argmax(count_columns)\n",
        "                word.append(DICT_2D[row][col])\n",
        "\n",
        "        print('Predicted Word using cross validation is (' + ''.join(word) + ') With accuracy of ' + str(round(np.mean(np.array(accuracies)), 2)))\n",
        "\n",
        "    def train_completely(self):\n",
        "\n",
        "        data = self.data_generator()\n",
        "        data = StandardScaler().fit(data).transform(data)\n",
        "        d, y = Person.unison_shuffled_copies(data, self.y_train)\n",
        "        clf = clone(self.CLF) # Copy of ideal Classifier\n",
        "        clf.fit(d, y)\n",
        "        return clf\n",
        "    \n",
        "    def predict_on_test(self):\n",
        "\n",
        "        clf = self.train_completely()\n",
        "\n",
        "        word = []\n",
        "        \n",
        "        data = self.data_generator(Train= False)\n",
        "        data = StandardScaler().fit(data).transform(data)\n",
        "        for i in range(5): \n",
        "\n",
        "            indexes = np.arange(self.trials)\n",
        "            test_index = np.arange(self.trials//5 * i, self.trials//5 * (i+1))\n",
        "            X_test = data[test_index]\n",
        "            letters_test = self.letters_test[test_index]\n",
        "            pred = clf.predict(X_test)\n",
        "\n",
        "            # Finding the letter:\n",
        "            simu = np.multiply(pred, letters_test).astype(np.int64)\n",
        "            simu[simu == 0][0] = 36\n",
        "            count = np.bincount(simu)\n",
        "            count[0] = 0\n",
        "            count[-1] = count[-1] - 1\n",
        "\n",
        "            if self.single_char:\n",
        "                confidence = count[np.argmax(count)] / np.sum(count)\n",
        "                letter = np.argmax(count) - 1\n",
        "                word.append(DICT_1D[letter])\n",
        "\n",
        "            else:\n",
        "                if not np.sum(count[13:]) == 0:\n",
        "                    raise ValueError\n",
        "                count_rows = count[7:12]\n",
        "                count_columns = count[1:6]\n",
        "\n",
        "                row = np.argmax(count_rows)\n",
        "                col = np.argmax(count_columns)\n",
        "\n",
        "                conf_row = count[np.argmax(count_rows)] / np.sum(count_rows)\n",
        "                conf_columns = count[np.argmax(count_columns)] / np.sum(count_columns)\n",
        "                word.append(DICT_2D[row][col])\n",
        "\n",
        "\n",
        "        print('Predicted Word is ' + ''.join(word))\n",
        "\n",
        "    def summary(self):\n",
        "\n",
        "        names = self.TOP_FEATURES_NAMES.split()\n",
        "        print('Using weighted accuracy as the metric, top 3 features for this person has been extracted')\n",
        "        print('These Features are: ')\n",
        "        print('1) ' + names[0])\n",
        "        print('2) ' + names[1])\n",
        "        print('3) ' + names[2])\n",
        "        print('3 different Classifiers were tested and the best is: ', end= '')\n",
        "        print(self.CLF)\n",
        "        print('The max CV accuracy is ' + str(round(self.MAX_ACC, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "k1vv4U2VI3al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Extracing Best Features with different WindowSizes"
      ],
      "metadata": {
        "id": "SR1FizIUTIPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persons256 = []\n",
        "for i in range(9):\n",
        "    person = Person(train_data=train_data[i], test_data= test_data[i], window= 256)\n",
        "    print('------')\n",
        "    print(i)\n",
        "    person.tune()\n",
        "    persons256.append(person)"
      ],
      "metadata": {
        "id": "coXhUzkxEDG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persons128 = []\n",
        "for i in range(9):\n",
        "    person = Person(train_data=train_data[i], test_data= test_data[i], window= 128)\n",
        "    print('------')\n",
        "    print(i)\n",
        "    person.tune()\n",
        "    persons128.append(person)"
      ],
      "metadata": {
        "id": "IuWjzmVUIzh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Validation Results"
      ],
      "metadata": {
        "id": "82A8ei90Jb6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "\n",
        "    print('Person numeber ' + str(i+1))\n",
        "    print('Size128: ', end= '')\n",
        "    persons128[i].predict_on_train()\n",
        "    print('Size256: ', end= '')\n",
        "    persons256[i].predict_on_train()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxOKmgt5Jhgf",
        "outputId": "7b98387c-009f-42b2-8cc7-84e7a756e35b"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person numeber 1\n",
            "Size128: Predicted Word using cross validation is (L4KAY) With accuracy of 0.59\n",
            "Size256: Predicted Word using cross validation is (E6FAN) With accuracy of 0.6\n",
            "Person numeber 2\n",
            "Size128: Predicted Word using cross validation is (LUKAS) With accuracy of 0.69\n",
            "Size256: Predicted Word using cross validation is (LUKAS) With accuracy of 0.68\n",
            "Person numeber 3\n",
            "Size128: Predicted Word using cross validation is (PHGAS) With accuracy of 0.58\n",
            "Size256: Predicted Word using cross validation is (HKKAS) With accuracy of 0.61\n",
            "Person numeber 4\n",
            "Size128: Predicted Word using cross validation is (HUKAS) With accuracy of 0.76\n",
            "Size256: Predicted Word using cross validation is (HUKAS) With accuracy of 0.7\n",
            "Person numeber 5\n",
            "Size128: Predicted Word using cross validation is (IUKAM) With accuracy of 0.65\n",
            "Size256: Predicted Word using cross validation is (IOKAS) With accuracy of 0.66\n",
            "Person numeber 6\n",
            "Size128: Predicted Word using cross validation is (KSKCS) With accuracy of 0.59\n",
            "Size256: Predicted Word using cross validation is (DCKAA) With accuracy of 0.57\n",
            "Person numeber 7\n",
            "Size128: Predicted Word using cross validation is (KIKAM) With accuracy of 0.66\n",
            "Size256: Predicted Word using cross validation is (JUKAS) With accuracy of 0.64\n",
            "Person numeber 8\n",
            "Size128: Predicted Word using cross validation is (HIKAS) With accuracy of 0.73\n",
            "Size256: Predicted Word using cross validation is (EIEAW) With accuracy of 0.65\n",
            "Person numeber 9\n",
            "Size128: Predicted Word using cross validation is (KUKAS) With accuracy of 0.68\n",
            "Size256: Predicted Word using cross validation is (WOKAS) With accuracy of 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Deciding Window Size"
      ],
      "metadata": {
        "id": "CQxiy6YLJBAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_persons = []\n",
        "\n",
        "final_persons.append(persons128[0])\n",
        "final_persons.append(persons256[1])\n",
        "final_persons.append(persons256[2])\n",
        "\n",
        "final_persons.append(persons128[3])\n",
        "final_persons.append(persons256[4])\n",
        "final_persons.append(persons256[5])\n",
        "\n",
        "final_persons.append(persons256[6])\n",
        "final_persons.append(persons128[7])\n",
        "final_persons.append(persons128[8])"
      ],
      "metadata": {
        "id": "SoeJX8AQJEqO"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Results"
      ],
      "metadata": {
        "id": "QIs9D4Q1iPhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Summary of selected featuers"
      ],
      "metadata": {
        "id": "ZZRABnGKW506"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "    person = final_persons[i]\n",
        "    print('Person Number ' + str(i+1) + ': ', end= '')\n",
        "    summary(person)\n",
        "    print('----------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPp6jl-BW9G6",
        "outputId": "197e49a5-8991-486a-df30-7938053194da"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person Number 1: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) fft_cor\n",
            "2) time_mean\n",
            "3) fft_var\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 35}, max_iter=5000)\n",
            "The max CV accuracy is 0.59\n",
            "----------\n",
            "Person Number 2: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_time\n",
            "2) bandpower\n",
            "3) time_mean\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 35}, max_iter=5000)\n",
            "The max CV accuracy is 0.68\n",
            "----------\n",
            "Person Number 3: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_time\n",
            "2) ft_median\n",
            "3) f_max\n",
            "3 different Classifiers were tested and the best is: SVC(C=0.3, class_weight={0: 1, 1: 5}, max_iter=5000)\n",
            "The max CV accuracy is 0.61\n",
            "----------\n",
            "Person Number 4: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_time\n",
            "2) bandpower\n",
            "3) ft_median\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 5}, max_iter=5000)\n",
            "The max CV accuracy is 0.76\n",
            "----------\n",
            "Person Number 5: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_time\n",
            "2) time_mean\n",
            "3) time_cor\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 5}, max_iter=5000)\n",
            "The max CV accuracy is 0.66\n",
            "----------\n",
            "Person Number 6: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_fft\n",
            "2) time_mean\n",
            "3) fft_mean\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 5}, max_iter=5000)\n",
            "The max CV accuracy is 0.57\n",
            "----------\n",
            "Person Number 7: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_time\n",
            "2) time_cor\n",
            "3) time_mean\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 5}, max_iter=5000)\n",
            "The max CV accuracy is 0.64\n",
            "----------\n",
            "Person Number 8: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_time\n",
            "2) fft_cor\n",
            "3) ft_mean_freq\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 5}, max_iter=5000)\n",
            "The max CV accuracy is 0.73\n",
            "----------\n",
            "Person Number 9: Using weighted accuracy as the metric, top 3 features for this person has been extracted\n",
            "These Features are: \n",
            "1) stack_time\n",
            "2) fft_mean\n",
            "3) f_max\n",
            "3 different Classifiers were tested and the best is: LogisticRegression(class_weight={0: 1, 1: 5}, max_iter=5000)\n",
            "The max CV accuracy is 0.68\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Validation Results"
      ],
      "metadata": {
        "id": "hByUUW8aJ8iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "    person = final_persons[i]\n",
        "    print('Person Number ' + str(i+1) + ': ', end= '')\n",
        "    person.predict_on_train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2QYNGh6AiHy",
        "outputId": "c611b7ad-c9d8-4652-c076-c5a6ce37a3c8"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person Number 1: Predicted Word using cross validation is (L4KAY) With accuracy of 0.59\n",
            "Person Number 2: Predicted Word using cross validation is (LUKAS) With accuracy of 0.68\n",
            "Person Number 3: Predicted Word using cross validation is (HKKAS) With accuracy of 0.61\n",
            "Person Number 4: Predicted Word using cross validation is (HUKAS) With accuracy of 0.76\n",
            "Person Number 5: Predicted Word using cross validation is (IOKAS) With accuracy of 0.66\n",
            "Person Number 6: Predicted Word using cross validation is (DCKAA) With accuracy of 0.57\n",
            "Person Number 7: Predicted Word using cross validation is (JUKAS) With accuracy of 0.64\n",
            "Person Number 8: Predicted Word using cross validation is (HIKAS) With accuracy of 0.73\n",
            "Person Number 9: Predicted Word using cross validation is (KUKAS) With accuracy of 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Results On Test Data"
      ],
      "metadata": {
        "id": "nGDiyajQiSeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "    person = final_persons[i]\n",
        "    print('Person Number ' + str(i+1) + ': ', end= '')\n",
        "    person.predict_on_test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eRN_IGMCBd_",
        "outputId": "dcce92de-d1c0-4aab-eb68-823677fc8b73"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person Number 1: Predicted Word is LDNHA\n",
            "Person Number 2: Predicted Word is LUKAS\n",
            "Person Number 3: Predicted Word is KUKAS\n",
            "Person Number 4: Predicted Word is KUKAS\n",
            "Person Number 5: Predicted Word is WATEQ\n",
            "Person Number 6: Predicted Word is WMUAA\n",
            "Person Number 7: Predicted Word is 2AZEM\n",
            "Person Number 8: Predicted Word is WATEM\n",
            "Person Number 9: Predicted Word is WATEP\n"
          ]
        }
      ]
    }
  ]
}